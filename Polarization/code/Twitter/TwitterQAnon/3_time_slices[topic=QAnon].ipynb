{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time slice analysis\n",
    "\n",
    "python=3.7\n",
    "\n",
    "|topic      |platform   |language   |\n",
    "|-----------|-----------|-----------|\n",
    "|QAnon  |Twitter    |en         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "name_suffix = \"[v2][debunking=keywords][lang=en][topic=QAnon][platform=Twitter]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load debunking retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102104, 83)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rt = pd.read_csv(f\"data/retweet{name_suffix}.csv\")\n",
    "df_rt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rt['text'] = df_rt['text'].astype(str)\n",
    "# Convert 'created_at' column to datetime\n",
    "df_rt['created_at'] = pd.to_datetime(df_rt['created_at'])\n",
    "# Extract date from 'created_at' column\n",
    "df_rt['date'] = df_rt['created_at'].dt.date\n",
    "# Group by date\n",
    "grouped_df = df_rt.groupby('date')\n",
    "\n",
    "# In each time slice, aggregate texts for each user\n",
    "time_slices = dict()\n",
    "for name, df in grouped_df:\n",
    "    time_slices[name] = df.groupby(by='author.username').agg(text=(\"text\", lambda x: ' '.join(set(x))))\n",
    "\n",
    "len(time_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time_slices from dict into DataFrame\n",
    "for k, v in time_slices.items():\n",
    "    v['date'] = k\n",
    "\n",
    "df_merge_slices = pd.concat(time_slices.values())\n",
    "df_merge_slices.sort_values(by='date', inplace=True)\n",
    "df_merge_slices['author.username'] = df_merge_slices.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1da819724c749ebb29afe816f835ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=9968), Label(value='0 / 9968'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import html\n",
    "def simple_text_cleaning(text):\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) \n",
    "    text = html.unescape(text) \n",
    "    text = re.sub(r'<.*?>+', '', text) \n",
    "    return text\n",
    "\n",
    "df_txt = df_merge_slices\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=10)\n",
    "df_txt['text_simply_cleaned'] = df_txt['text'].parallel_apply(simple_text_cleaning)\n",
    "\n",
    "df_merge_slices = df_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 99679 entries, 40AcresBuilt to zorrooro\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   text                 99679 non-null  object\n",
      " 1   date                 99679 non-null  object\n",
      " 2   author.username      99679 non-null  object\n",
      " 3   text_simply_cleaned  99679 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_merge_slices.to_csv(f\"data/time_slices{name_suffix}.csv\", index=False)\n",
    "df_merge_slices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of texts: 6911\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts = pd.DataFrame(df_merge_slices['text_simply_cleaned'].drop_duplicates())\n",
    "texts = texts.rename(columns={'text_simply_cleaned': 'text'})   \n",
    "texts.to_csv(f\"data/texts_of_time_slices{name_suffix}.csv\", index=False)\n",
    "print(f\"number of texts: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6911 entries, 0 to 6910\n",
      "Data columns (total 3 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   text                     6911 non-null   object \n",
      " 1   perspective_api_results  6887 non-null   object \n",
      " 2   toxicity                 6887 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 162.1+ KB\n"
     ]
    }
   ],
   "source": [
    "texts = pd.read_csv(f\"data/toxicity_of_texts_of_time_slices{name_suffix}.csv\")\n",
    "\n",
    "def get_score_from_json(x):\n",
    "    \n",
    "    if pd.isna(x) or not x:\n",
    "        return None\n",
    "    s = re.search(\"'score': {'value': (.+?),\", x)\n",
    "    return float(s.group(1))\n",
    "\n",
    "texts['toxicity'] = texts['perspective_api_results'].apply(get_score_from_json)\n",
    "texts.to_csv(f\"data/toxicity_of_texts_of_time_slices{name_suffix}.csv\", index=False)\n",
    "texts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiments detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'neu', 'pos', 'compound']\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "example = nltk_analyzer.polarity_scores(\"\")\n",
    "sentiment_names = list(example.keys())\n",
    "print(sentiment_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5837d54a9a44408988db153d1a4887b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=692), Label(value='0 / 692'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6911 entries, 0 to 6910\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   text                     6911 non-null   object \n",
      " 1   perspective_api_results  6887 non-null   object \n",
      " 2   toxicity                 6887 non-null   float64\n",
      " 3   neg                      6911 non-null   float64\n",
      " 4   neu                      6911 non-null   float64\n",
      " 5   pos                      6911 non-null   float64\n",
      " 6   compound                 6911 non-null   float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 378.1+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=10)\n",
    "\n",
    "# user_texts.drop(columns=sentiment_names, inplace=True)\n",
    "\n",
    "result = texts['text'].astype(str).parallel_apply(nltk_analyzer.polarity_scores)\n",
    "result = pd.DataFrame(result.tolist())\n",
    "\n",
    "texts = pd.concat([texts, result], axis=1)\n",
    "texts.to_csv(f\"data/scores_of_time_slices_texts{name_suffix}.csv\", index=False)\n",
    "texts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize daily datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99679 entries, 0 to 99678\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   text                 99679 non-null  object \n",
      " 1   date                 99679 non-null  object \n",
      " 2   author.username      99679 non-null  object \n",
      " 3   text_simply_cleaned  99679 non-null  object \n",
      " 4   neg                  99679 non-null  float64\n",
      " 5   neu                  99679 non-null  float64\n",
      " 6   pos                  99679 non-null  float64\n",
      " 7   compound             99679 non-null  float64\n",
      " 8   toxicity             99593 non-null  float64\n",
      "dtypes: float64(5), object(4)\n",
      "memory usage: 6.8+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts.index = texts['text']\n",
    "cols = sentiment_names + ['toxicity']\n",
    "score_dict = dict(texts[cols].T.items())\n",
    "\n",
    "def mapping_texts(x):\n",
    "    if x in score_dict.keys():\n",
    "        return score_dict[x]\n",
    "    else:\n",
    "        return pd.Series({k:None for k in cols})\n",
    "\n",
    "df_merge_slices = pd.read_csv(f\"data/time_slices{name_suffix}.csv\")\n",
    "df_merge_slices[cols] = df_merge_slices['text_simply_cleaned'].apply(mapping_texts)\n",
    "df_merge_slices.to_csv(f\"data/time_slices{name_suffix}.csv\", index=False)\n",
    "df_merge_slices.to_csv(f\"/mnt/data/shared/time_slice_data[sentiment=nltk]/time_slices{name_suffix}.csv\", index=False)\n",
    "df_merge_slices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(df:pd.DataFrame, scores:'list[str]'):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df_groupby_date = df.groupby('date')\n",
    "\n",
    "    daily_user_count = df_groupby_date['author.username'].nunique()\n",
    "    score_none_count = df_groupby_date[scores].apply(lambda x: x.isnull().sum())\n",
    "    daily_mean = df_groupby_date[scores].mean()\n",
    "    daily_median = df_groupby_date[scores].median()\n",
    "\n",
    " \n",
    "    def mean_no_extreme(df:pd.DataFrame):\n",
    "        q1 = df[scores].quantile(0.25)\n",
    "        q3 = df[scores].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        df_no_extreme = df[~((df[scores] < (q1 - 1.5 * iqr)) | (df[scores] > (q3 + 1.5 * iqr)))]\n",
    "        return df_no_extreme[scores].mean()\n",
    "    \n",
    "    daily_mean_no_extreme = df_groupby_date.apply(mean_no_extreme)\n",
    "\n",
    "\n",
    "    daily_data = {\n",
    "        'date': daily_user_count.index,\n",
    "        'user_count': daily_user_count.values\n",
    "    }\n",
    "    _none_count = {score + '_none_count': score_none_count[score] for score in scores}\n",
    "    _mean = {score + '_mean': daily_mean[score].values for score in scores}\n",
    "    _median = {score + '_media': daily_median[score].values for score in scores}\n",
    "    _mean_no_extreme = {score + '_mean_no_extreme': daily_mean_no_extreme[score].values for score in scores}\n",
    "\n",
    "\n",
    "    daily_data = pd.DataFrame({**daily_data, **_none_count, **_mean, **_median, **_mean_no_extreme})\n",
    "\n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_count</th>\n",
       "      <th>neg_none_count</th>\n",
       "      <th>neu_none_count</th>\n",
       "      <th>pos_none_count</th>\n",
       "      <th>compound_none_count</th>\n",
       "      <th>toxicity_none_count</th>\n",
       "      <th>neg_mean</th>\n",
       "      <th>neu_mean</th>\n",
       "      <th>pos_mean</th>\n",
       "      <th>compound_mean</th>\n",
       "      <th>toxicity_mean</th>\n",
       "      <th>neg_media</th>\n",
       "      <th>neu_media</th>\n",
       "      <th>pos_media</th>\n",
       "      <th>compound_media</th>\n",
       "      <th>toxicity_media</th>\n",
       "      <th>neg_mean_no_extreme</th>\n",
       "      <th>neu_mean_no_extreme</th>\n",
       "      <th>pos_mean_no_extreme</th>\n",
       "      <th>compound_mean_no_extreme</th>\n",
       "      <th>toxicity_mean_no_extreme</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.273521</td>\n",
       "      <td>0.718863</td>\n",
       "      <td>0.007616</td>\n",
       "      <td>-0.794732</td>\n",
       "      <td>0.352163</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.7110</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.8317</td>\n",
       "      <td>0.338998</td>\n",
       "      <td>0.312964</td>\n",
       "      <td>0.687036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.842908</td>\n",
       "      <td>0.347779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.209867</td>\n",
       "      <td>0.768200</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>-0.707317</td>\n",
       "      <td>0.307885</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.7225</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>-0.8176</td>\n",
       "      <td>0.349975</td>\n",
       "      <td>0.209867</td>\n",
       "      <td>0.760207</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>-0.833765</td>\n",
       "      <td>0.307885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-03</th>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.172897</td>\n",
       "      <td>0.821862</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>-0.628976</td>\n",
       "      <td>0.281776</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.6662</td>\n",
       "      <td>0.307163</td>\n",
       "      <td>0.150571</td>\n",
       "      <td>0.821862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.733588</td>\n",
       "      <td>0.303564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-04</th>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168592</td>\n",
       "      <td>0.788750</td>\n",
       "      <td>0.042763</td>\n",
       "      <td>-0.605432</td>\n",
       "      <td>0.317561</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.6662</td>\n",
       "      <td>0.307163</td>\n",
       "      <td>0.168592</td>\n",
       "      <td>0.788750</td>\n",
       "      <td>0.042763</td>\n",
       "      <td>-0.605432</td>\n",
       "      <td>0.317561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-05</th>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240205</td>\n",
       "      <td>0.751589</td>\n",
       "      <td>0.008192</td>\n",
       "      <td>-0.808037</td>\n",
       "      <td>0.464101</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.7320</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.9075</td>\n",
       "      <td>0.548233</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.907500</td>\n",
       "      <td>0.548233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  user_count  neg_none_count  neu_none_count  \\\n",
       "date                                                                \n",
       "2020-04-01 2020-04-01          73               0               0   \n",
       "2020-04-02 2020-04-02          30               0               0   \n",
       "2020-04-03 2020-04-03          29               0               0   \n",
       "2020-04-04 2020-04-04          76               0               0   \n",
       "2020-04-05 2020-04-05          73               0               0   \n",
       "\n",
       "            pos_none_count  compound_none_count  toxicity_none_count  \\\n",
       "date                                                                   \n",
       "2020-04-01               0                    0                    0   \n",
       "2020-04-02               0                    0                    0   \n",
       "2020-04-03               0                    0                    0   \n",
       "2020-04-04               0                    0                    0   \n",
       "2020-04-05               0                    0                    0   \n",
       "\n",
       "            neg_mean  neu_mean  pos_mean  compound_mean  toxicity_mean  \\\n",
       "date                                                                     \n",
       "2020-04-01  0.273521  0.718863  0.007616      -0.794732       0.352163   \n",
       "2020-04-02  0.209867  0.768200  0.021900      -0.707317       0.307885   \n",
       "2020-04-03  0.172897  0.821862  0.005241      -0.628976       0.281776   \n",
       "2020-04-04  0.168592  0.788750  0.042763      -0.605432       0.317561   \n",
       "2020-04-05  0.240205  0.751589  0.008192      -0.808037       0.464101   \n",
       "\n",
       "            neg_media  neu_media  pos_media  compound_media  toxicity_media  \\\n",
       "date                                                                          \n",
       "2020-04-01      0.289     0.7110     0.0000         -0.8317        0.338998   \n",
       "2020-04-02      0.235     0.7225     0.0355         -0.8176        0.349975   \n",
       "2020-04-03      0.133     0.8670     0.0000         -0.6662        0.307163   \n",
       "2020-04-04      0.133     0.8670     0.0000         -0.6662        0.307163   \n",
       "2020-04-05      0.268     0.7320     0.0000         -0.9075        0.548233   \n",
       "\n",
       "            neg_mean_no_extreme  neu_mean_no_extreme  pos_mean_no_extreme  \\\n",
       "date                                                                        \n",
       "2020-04-01             0.312964             0.687036             0.000000   \n",
       "2020-04-02             0.209867             0.760207             0.021900   \n",
       "2020-04-03             0.150571             0.821862             0.000000   \n",
       "2020-04-04             0.168592             0.788750             0.042763   \n",
       "2020-04-05             0.268000             0.732000             0.000000   \n",
       "\n",
       "            compound_mean_no_extreme  toxicity_mean_no_extreme  \n",
       "date                                                            \n",
       "2020-04-01                 -0.842908                  0.347779  \n",
       "2020-04-02                 -0.833765                  0.307885  \n",
       "2020-04-03                 -0.733588                  0.303564  \n",
       "2020-04-04                 -0.605432                  0.317561  \n",
       "2020-04-05                 -0.907500                  0.548233  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = df_merge_slices\n",
    "data = pd.read_csv(f\"data/time_slices{name_suffix}.csv\")\n",
    "scores = sentiment_names + ['toxicity']\n",
    "daily_statistics = calculate_statistics(data, scores=scores)\n",
    "daily_statistics.to_csv(f\"data/daily_statistics{name_suffix}.csv\", index=False)\n",
    "daily_statistics.to_csv(f\"/mnt/data/shared/time_slice_data[sentiment=nltk]/daily_statistics{name_suffix}.csv\", index=False)\n",
    "daily_statistics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 384 entries, 2020-04-01 to 2021-04-30\n",
      "Data columns (total 22 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   date                      384 non-null    datetime64[ns]\n",
      " 1   user_count                384 non-null    int64         \n",
      " 2   neg_none_count            384 non-null    int64         \n",
      " 3   neu_none_count            384 non-null    int64         \n",
      " 4   pos_none_count            384 non-null    int64         \n",
      " 5   compound_none_count       384 non-null    int64         \n",
      " 6   toxicity_none_count       384 non-null    int64         \n",
      " 7   neg_mean                  384 non-null    float64       \n",
      " 8   neu_mean                  384 non-null    float64       \n",
      " 9   pos_mean                  384 non-null    float64       \n",
      " 10  compound_mean             384 non-null    float64       \n",
      " 11  toxicity_mean             384 non-null    float64       \n",
      " 12  neg_media                 384 non-null    float64       \n",
      " 13  neu_media                 384 non-null    float64       \n",
      " 14  pos_media                 384 non-null    float64       \n",
      " 15  compound_media            384 non-null    float64       \n",
      " 16  toxicity_media            384 non-null    float64       \n",
      " 17  neg_mean_no_extreme       384 non-null    float64       \n",
      " 18  neu_mean_no_extreme       384 non-null    float64       \n",
      " 19  pos_mean_no_extreme       384 non-null    float64       \n",
      " 20  compound_mean_no_extreme  384 non-null    float64       \n",
      " 21  toxicity_mean_no_extreme  384 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(15), int64(6)\n",
      "memory usage: 69.0 KB\n"
     ]
    }
   ],
   "source": [
    "daily_statistics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395\n"
     ]
    }
   ],
   "source": [
    "# Calculate days from the earliest date to the latest\n",
    "delta = daily_statistics['date'][-1] - daily_statistics['date'][0]\n",
    "print(delta.days + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
