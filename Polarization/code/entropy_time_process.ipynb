{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 The preprocessing procedure.\n",
    "Import the necessary libraries required for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=8)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import itertools\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "import scipy.stats as stats\n",
    "from adjustText import adjust_text\n",
    "from cliffs_delta import cliffs_delta\n",
    "from statsmodels.nonparametric.bandwidths import bw_silverman\n",
    "import diptest\n",
    "\n",
    "# Add a label to the image.\n",
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l','m','n','o','p','q','r','s','t','u']\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 16,  \n",
    "    'axes.labelsize': 16,  \n",
    "    'xtick.labelsize': 14, \n",
    "    'ytick.labelsize': 14, \n",
    "    'figure.dpi' : 300 ,\n",
    "    'font.weight' : 'normal'  \n",
    "})\n",
    "\n",
    "line_color = {'Democrats': 'blue', 'Republicans': 'red'}\n",
    "scatter_color = {'Democrats': '#66CCFF', 'Republicans': '#FF3333'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Word Entropy\n",
    "def entropy(text):\n",
    "    wordlist = nltk.word_tokenize(text.lower())\n",
    "    freq_dict = nltk.probability.FreqDist(wordlist)\n",
    "    freq = np.array(list(freq_dict.values())) / len(wordlist)\n",
    "    return -(freq * np.log2(freq)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset that aggregated texts by authors.\n",
    "dataset = pd.read_csv(\"userDataset.csv\")\n",
    "\n",
    "# dataset['entropy'] = dataset['text'].map(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 The phenomenon of polarization across different platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the fig4\n",
    "jointDataset = dataset.melt(id_vars=['party','Theme','Platform', \"core_status\"], var_name='ValueType', value_name='Value',value_vars=['pessimism','toxicity'],)\n",
    "jointDataset['Platform'] =  jointDataset['Platform'] + ' (' + jointDataset['ValueType'] + ')'\n",
    "\n",
    "g = sns.FacetGrid(jointDataset, row='Platform', col='Theme',\n",
    "                  row_order=['Twitter (toxicity)','Reddit (toxicity)', \n",
    "                             'Twitter (pessimism)', 'Reddit (pessimism)',],\n",
    "                    height=4, aspect=1.5 , sharex='col', sharey=False,\n",
    "                     margin_titles=True)\n",
    "\n",
    "g.set_titles(col_template=\"{col_name}\\n\", row_template=\"{row_name}\", size=20, fontweight='bold')\n",
    "\n",
    "g.map_dataframe(sns.violinplot, x=\"core_status\", y=\"Value\", \n",
    "                hue='party',hue_order = [\"Democrats\", \"Republicans\"],\n",
    "                palette=scatter_color,\n",
    "                order = ['1-degree', '2-core'],\n",
    "                dodge=True)\n",
    "\n",
    "g.fig.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "\n",
    "for text_obj in g._margin_titles_texts:\n",
    "    original_title = text_obj.get_text()  \n",
    "    text_obj.set_text(original_title.split()[0])  \n",
    "\n",
    "for index, ax in enumerate(g.axes.flat):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.set_ylabel('')\n",
    "    if index in [0, 3]:\n",
    "        ax.set_ylabel('Language toxicity', fontsize=20, fontweight='bold')\n",
    "    if index in [6, 9]:\n",
    "        ax.set_ylabel('Pessimism', fontsize=20, fontweight='bold')\n",
    "    if index in [9, 10 ,11]:\n",
    "        for label in ax.get_xticklabels():\n",
    "            label.set_fontweight('bold')  \n",
    "\n",
    "    ax.text(0, 1.1, letters[index], transform=ax.transAxes, \n",
    "        fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "g.add_legend(label_order=['Republicans','Democrats'], loc='upper center', title=None, bbox_to_anchor=(0.35, 1.04), ncol=2, fontsize=20, markerscale=2.5)\n",
    "g.set_xlabels('', fontsize=20, fontweight='bold')\n",
    "plt.savefig('fig4.png', bbox_inches='tight')\n",
    "plt.savefig('fig4.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 The phenomenon of polarization over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to read dataset over time.\n",
    "TopicsWithDatasetPath = {\n",
    "    '2016 U.S. presidential election(Twitter)' : 'Twitter/daily_statistics[pessimism][debunking=keywords][lang=en][topic=POTUS2016][platform=Twitter].csv',\n",
    "    '2020 U.S. presidential election(Twitter)' : 'Twitter/daily_statistics[pessimism][debunking=keywords][lang=en][topic=POTUS2020][platform=Twitter].csv',\n",
    "    'QAnon(Twitter)'     : 'Twitter/daily_statistics[pessimism][debunking=keywords][lang=en][topic=QAnon][platform=Twitter].csv',\n",
    "    '2016 U.S. presidential election(Reddit)'  : 'Reddit/daily_statistics[pessimism][debunking=keywords][lang=en][topic=POTUS2016][platform=Reddit].csv',\n",
    "    '2020 U.S. presidential election(Reddit)'  : 'Reddit/daily_statistics[pessimism][debunking=keywords][lang=en][topic=POTUS2020][platform=Reddit].csv',\n",
    "    'QAnon(Reddit)'      : 'Reddit/daily_statistics[pessimism][debunking=keywords][lang=en][topic=QAnon][platform=Reddit].csv',\n",
    "}\n",
    "\n",
    "TopicsWithDatasetRaw = {topic: pd.read_csv(datasetPath) for topic, datasetPath in TopicsWithDatasetPath.items()}\n",
    "ColumnsNames = ['date','user_count','compound_mean_no_extreme','toxicity_mean_no_extreme','pessimism_mean_no_extreme']\n",
    "RenamedColumns = {\n",
    "    'date':'date',\n",
    "    'user_count':'user_count',\n",
    "    'compound_mean_no_extreme':'sentiment',\n",
    "    'toxicity_mean_no_extreme':'toxicity',\n",
    "    'pessimism_mean_no_extreme':'pessimism'\n",
    "}\n",
    "\n",
    "TopicsWithDataset = {topic: datasetRaw[ColumnsNames].rename(columns=RenamedColumns) for topic, datasetRaw in TopicsWithDatasetRaw.items()}\n",
    "for topic, dataset in TopicsWithDataset.items():\n",
    "    dataset['user_count'] = dataset['user_count'].map(np.log10)\n",
    "    dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "\n",
    "TopicsWithDataset['2020 U.S. presidential election(Twitter)'] = TopicsWithDataset['2020 U.S. presidential election(Twitter)'][TopicsWithDataset['2020 U.S. presidential election(Twitter)']['date'] >= pd.to_datetime('2020-09-06')]\n",
    "TopicsWithDatasetInDate = {topic: dataset.set_index('date') for topic, dataset in TopicsWithDataset.items()}\n",
    "\n",
    "\n",
    "# Alignment\n",
    "ThemeType = ['2016 U.S. presidential election','2020 U.S. presidential election',\"QAnon\"]\n",
    "PlatformType = ['Reddit','Twitter']\n",
    "PartyType = ['Democrats','Republicans']\n",
    "for theme in ThemeType:\n",
    "    maxData = min(TopicsWithDatasetInDate[f'{theme}(Reddit)'].index.max(), TopicsWithDatasetInDate[f'{theme}(Twitter)'].index.max())\n",
    "    minData = max(TopicsWithDatasetInDate[f'{theme}(Reddit)'].index.min(), TopicsWithDatasetInDate[f'{theme}(Twitter)'].index.min())\n",
    "    TopicsWithDatasetInDate[f'{theme}(Reddit)'] = TopicsWithDatasetInDate[f'{theme}(Reddit)'][minData:maxData]\n",
    "    TopicsWithDatasetInDate[f'{theme}(Twitter)'] = TopicsWithDatasetInDate[f'{theme}(Twitter)'][minData:maxData]\n",
    "\n",
    "# Due to the presence of gaps in the middle, the non-gap portions of time were extracted to ensure consistency.\n",
    "TopicsWithDatasetInDate = {topic: dataset.reset_index() for topic, dataset in TopicsWithDatasetInDate.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PointDate = {\n",
    "    '2016 U.S. presidential election(Twitter)' : pd.to_datetime(\"2016-11-8\"),\n",
    "    '2020 U.S. presidential election(Twitter)' : pd.to_datetime(\"2020-11-3\"),\n",
    "    'QAnon(Twitter)' : pd.to_datetime(\"2021-1-6\"),\n",
    "    '2016 U.S. presidential election(Reddit)' : pd.to_datetime(\"2016-11-8\"),\n",
    "    '2020 U.S. presidential election(Reddit)' : pd.to_datetime(\"2020-11-3\"),\n",
    "    'QAnon(Reddit)' : pd.to_datetime(\"2021-1-6\"),\n",
    "}\n",
    "# end\n",
    "days_d = {}\n",
    "keyDays = {}\n",
    "TopicsWithDatasetInDateIndex = {topic: dataset.set_index('date') for topic, dataset in TopicsWithDatasetInDate.items()}\n",
    "\n",
    "for topic, dataset in TopicsWithDatasetInDateIndex.items():\n",
    "    target_date = PointDate[topic]\n",
    "    # Head 7 days\n",
    "    d =  dataset\n",
    "    days = d.index\n",
    "    times = 2\n",
    "    start_date = target_date - pd.Timedelta(days=7)\n",
    "    end_date = target_date + pd.Timedelta(days=7)\n",
    "    middle = [{\n",
    "        'date' : target_date,\n",
    "        'toxicity' : d.loc[start_date:end_date].median()['toxicity'],\n",
    "        'pessimism' : d.loc[start_date:end_date].median()['pessimism'] \n",
    "    }]\n",
    "\n",
    "    # Last 7 days\n",
    "    s = end_date\n",
    "    ends = []\n",
    "    i = 0\n",
    "    while s + pd.Timedelta(days=1) in days and i < times:\n",
    "        i += 1\n",
    "        e = s + pd.Timedelta(days=7)\n",
    "        s = s + pd.Timedelta(days=1)\n",
    "        t = s + pd.Timedelta(days=15)\n",
    "        ends.append({\n",
    "            'date' : t,\n",
    "            'toxicity' : d.loc[s:e].median()['toxicity'],\n",
    "            'pessimism' : d.loc[s:e].median()['pessimism']\n",
    "        })\n",
    "        s = e\n",
    "    \n",
    "    starts = []\n",
    "    e = start_date\n",
    "    i = 0\n",
    "    while e - pd.Timedelta(days=1) in days and i < times:\n",
    "        i += 1\n",
    "        s = e - pd.Timedelta(days=7)\n",
    "        e = e - pd.Timedelta(days=1)\n",
    "        t = s - pd.Timedelta(days=15)\n",
    "        starts.append({\n",
    "            'date' : t,\n",
    "            'toxicity' : d.loc[s:e].median()['toxicity'],\n",
    "            'pessimism' : d.loc[s:e].median()['pessimism']\n",
    "        })\n",
    "        e = s\n",
    "\n",
    "    days_d[topic] = starts + middle + ends\n",
    "    keyDays[topic] = {\n",
    "        'date' : target_date,\n",
    "        'toxicity' : d.loc[start_date:end_date]['toxicity'],\n",
    "        'pessimism' : d.loc[start_date:end_date]['pessimism'] \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate k and b by computing the linear fit.\n",
    "a = list(itertools.chain.from_iterable(days_d.values()))\n",
    "b = list(map(lambda x: (x['toxicity'], x['pessimism']), a))\n",
    "toxicity, pessimism = zip(*b)\n",
    "k, b = np.polyfit(pessimism, toxicity, deg=1)\n",
    "x = np.linspace(0.055, 0.075, 1000)\n",
    "y = k*x + b\n",
    "\n",
    "v = list(PointDate.values())\n",
    "a = {topic : pd.DataFrame(data) for topic, data in days_d.items()}\n",
    "rangeday = [[data['date'].max(), data['date'].min()] for topic, data in a.items()]\n",
    "rangeday = list(itertools.chain.from_iterable(rangeday))\n",
    "a = pd.concat(a.values(), keys=a.keys())\n",
    "a = a.reset_index(level=0)\n",
    "a = a.rename(columns={\"level_0\": \"topic\"})\n",
    "a = a.sort_values('date')\n",
    "\n",
    "labeldays = a[a['date'].isin(v)]\n",
    "rangedays = a[a['date'].isin(rangeday)]\n",
    "\n",
    "\n",
    "# Draw the Fig6\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(a, y='toxicity', x='pessimism', hue='topic', s=150,\n",
    "                hue_order=['2016 U.S. presidential election(Twitter)','2020 U.S. presidential election(Twitter)','QAnon(Twitter)','2016 U.S. presidential election(Reddit)','2020 U.S. presidential election(Reddit)','QAnon(Reddit)'])\n",
    "# Toxicity / Pessimism\n",
    "sns.lineplot(a, y='toxicity', x='pessimism', hue='topic', alpha=0.5, legend=False, sort=False, linewidth=4,\n",
    "             hue_order=['2016 U.S. presidential election(Twitter)','2020 U.S. presidential election(Twitter)','QAnon(Twitter)','2016 U.S. presidential election(Reddit)','2020 U.S. presidential election(Reddit)','QAnon(Reddit)'])\n",
    "\n",
    "sns.lineplot(x=x, y=y, linestyle=\"--\", linewidth=4, color=\"red\")\n",
    "plt.text(0.063, 0.28, \"Reddit\", fontsize=20, fontweight=\"bold\", bbox=dict(facecolor='white', edgecolor='red', boxstyle='round, pad=0.5', linewidth=3))\n",
    "plt.text(0.054, 0.20, \"Twitter\", fontsize=20, fontweight=\"bold\", bbox=dict(facecolor='white', edgecolor='red', boxstyle='round, pad=0.5',linewidth=3))\n",
    "\n",
    "_ = [plt.text(x, y, s.strftime('%Y-%m-%d'), va='center') for x, y ,s in zip(rangedays['pessimism'].tolist(), rangedays['toxicity'].tolist(), rangedays['date'].tolist())]\n",
    "_ += [plt.text(x, y, \"January 6 United States Capitol attack\", fontsize=14, fontweight='bold',  va='center') for x, y ,topic in zip(labeldays['pessimism'].tolist(), labeldays['toxicity'].tolist(), labeldays['topic'].tolist()) if topic.startswith(\"QAnon\") == True]\n",
    "_ += [plt.text(x, y, f\"{topic[:4]} Election Day\", fontsize=14, fontweight='bold', va='center') for x, y ,topic in zip(labeldays['pessimism'].tolist(), labeldays['toxicity'].tolist(), labeldays['topic'].tolist()) if topic.startswith(\"QAnon\") != True]\n",
    "_ = adjust_text(_, x=a['pessimism'], y=a['toxicity'], expand=(1.2, 1.4), arrowprops=dict(arrowstyle='->', color='black'), prevent_crossings=True, min_arrow_len=7)\n",
    "\n",
    "\n",
    "plt.xlabel('Pessimism', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Language Toxicity' , fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.savefig('fig6.pdf')\n",
    "plt.savefig('fig6.png')\n",
    "\n",
    "# In reality, the annotation was done manually, but the current code employs automatic annotation (which may be somewhat disorganized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 The phenomenon of polarization in Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 1 Entropy minimal interval for 50% of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeProp(dataset, inf, sup):\n",
    "    return ((dataset > inf) & (dataset < sup)).sum() / len(dataset)\n",
    "\n",
    "def FindIntervals(dataset, length):\n",
    "    sup = dataset.max()\n",
    "    ret = []\n",
    "    for beg in np.arange(0, sup-length, 0.1):\n",
    "        occ = ComputeProp(dataset, beg, beg+length)\n",
    "        if occ > 0.5:\n",
    "            ret.append({\n",
    "                \"begin\" :np.round(beg,1),\n",
    "                \"end\": np.round(beg+length,1),\n",
    "                \"length\" : np.round(length,1),\n",
    "                \"occ\" : np.round(occ,4),\n",
    "            })\n",
    "    return ret\n",
    "\n",
    "def FindMinimumInterval(dataset):\n",
    "    sup = dataset.max()\n",
    "    suc = []\n",
    "    for length in np.arange(0, sup, 0.1):\n",
    "        suc += FindIntervals(dataset, length)\n",
    "        if len(suc) >= 1:\n",
    "            break\n",
    "    return suc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = dataset.groupby([\"Theme\",\"Platform\"]).apply(lambda x: pd.Series({\n",
    "    \"x\" : FindMinimumInterval(x['entropy'])[0]\n",
    "}))\n",
    "ret = ret.reset_index()\n",
    "ret[[\"begin\", \"end\", \"length\", \"occ\"]] = pd.json_normalize(ret['x'])\n",
    "ret = ret.drop(columns=[\"x\"])\n",
    "ret.to_csv(\"Entropy minimal interval.csv\") # Entropy minimal interval for 50% of users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 2 Entropy polarization across different platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing on this 50% of users as the main subject, explore the phenomenon of entropy polarization.\n",
    "rangeList = pd.read_csv(\"Entropy minimal interval.csv\")\n",
    "range_dict = rangeList.set_index([\"Theme\",\"Platform\"]).to_dict()\n",
    "def filter_entropy_inver(x):\n",
    "    theme = x['Theme']\n",
    "    platform = x['Platform']\n",
    "    ret = (x['entropy'] > range_dict['begin'][(theme, platform)]) & (x['entropy'] < range_dict['end'][(theme, platform)])\n",
    "    return ret\n",
    "dataset['isInEntropy'] = dataset.apply(filter_entropy_inver, axis=1)\n",
    "inDataset = dataset[dataset['isInEntropy'] == True]\n",
    "\n",
    "subd = dataset.groupby([\"Theme\",'Platform',\"party\"]).apply(lambda x:\n",
    "    pd.Series({\n",
    "        \"toxicity\":filter_entropy_inver(x)['toxicity'].median(),\n",
    "        \"pessimism\":filter_entropy_inver(x)['pessimism'].median(),\n",
    "        \"entropy\":filter_entropy_inver(x)['entropy'].median(),\n",
    "    })\n",
    ")\n",
    "labels_data = subd.sort_values(['Platform',\"Theme\",\"party\"],ascending=[False, True, False]).values\n",
    "subd.sort_values(['Platform',\"Theme\",\"party\"],ascending=[False, True, False])\n",
    "\n",
    "subd['Size'] = subd['entropy'].map(lambda x: np.exp2(x))\n",
    "subd['Size'] = subd['Size'] / subd['Size'].min() * 500\n",
    "subd.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Fig7\n",
    "\n",
    "globalMinSize = subd['Size'].min()\n",
    "globalMaxSize = subd['Size'].max()\n",
    "\n",
    "g = sns.FacetGrid(subd.reset_index(), row='Platform', col='Theme', hue='party', row_order=['Twitter','Reddit'], palette=scatter_color, height=3, aspect=1.8, sharey=True, sharex=True, margin_titles=True)\n",
    "g.set_titles(col_template=\"{col_name}\\n\", row_template=\"{row_name}\", size=20, fontweight='bold')\n",
    "g.map_dataframe( plt.scatter, \n",
    "                x='toxicity',\n",
    "                y='pessimism',\n",
    "                s=\"Size\",\n",
    "                alpha=0.8,\n",
    "                )\n",
    "\n",
    "g.fig.subplots_adjust(wspace=0.18, hspace=0.32)\n",
    "\n",
    "for index, ax in enumerate(g.axes.flat):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", bottom=True, labelbottom=True)\n",
    "    ax.tick_params(axis=\"y\", which=\"both\", left=True, labelleft=True)\n",
    "\n",
    "    ax.set_ylim((0.04,0.09))\n",
    "    ax.set_xlim((0.10,0.4))\n",
    "    toxicity, pessimism, entropy = labels_data[2*index]\n",
    "    _ = [ax.text(x=toxicity, y=pessimism, s=f\"{entropy:.1f}\", fontsize=16, fontweight='bold', horizontalalignment='center',  # 水平对齐方式，可以是 'left', 'center', 'right'\n",
    "    verticalalignment='center') ]\n",
    "    toxicity, pessimism, entropy = labels_data[2*index+1]\n",
    "    _ += [ax.text(x=toxicity, y=pessimism, s=f\"{entropy:.1f}\", fontsize=16, fontweight='bold', horizontalalignment='center',  # 水平对齐方式，可以是 'left', 'center', 'right'\n",
    "    verticalalignment='center')] \n",
    "    ax.text(0, 1.15, letters[index], transform=ax.transAxes, \n",
    "            fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "    \n",
    "    adjust_text(_, expand=(2, 2.5), arrowprops=dict(arrowstyle='->', color='black'), ax=ax)\n",
    "\n",
    "legend_data = {\n",
    "    'Democrats' : g._legend_data['Democrats'],\n",
    "    'Republicans' : g._legend_data['Republicans'],\n",
    "}\n",
    "g.set_axis_labels('Language toxicity','Pessimism', fontsize=20, fontweight='bold')\n",
    "\n",
    "g.add_legend(legend_data=legend_data, label_order=['Republicans','Democrats'], loc='upper center', title=\"\", bbox_to_anchor=(0.35, 1.11), ncol=2, fontsize=20, markerscale=0.5)\n",
    "plt.savefig('Fig7.png', bbox_inches='tight')\n",
    "plt.savefig('Fig7.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 3 Toxicity With Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['entropyRound'] = dataset['entropy'].round(1)\n",
    "mediandatatset = dataset.groupby(['Theme',\"Platform\",\"party\",\"entropyRound\"]).apply(lambda x: pd.Series({\n",
    "    \"toxicity\" : x['toxicity'].dropna().median(),\n",
    "    \"pessimism\" : x['pessimism'].dropna().median()\n",
    "}))\n",
    "\n",
    "def eval_deg1(x, y) :\n",
    "    (a, b) = np.polyfit(x, y, deg=1)\n",
    "    y_pred = a*x + b\n",
    "    r = np.corrcoef(y, y_pred)[0, 1]\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    return r, r2\n",
    "\n",
    "# Figure 8\n",
    "reval = mediandatatset.reset_index().dropna().groupby(['Theme',\"Platform\",\"party\"]).apply(lambda x: \n",
    "    pd.Series({\n",
    "        \"r\" : eval_deg1(x['toxicity'], x['entropyRound'])[0],\n",
    "        \"r2\" : eval_deg1(x['toxicity'], x['entropyRound'])[1],\n",
    "    })\n",
    ")\n",
    "resorted = reval.reset_index().sort_values(['Platform','Theme','party'], ascending=[False, True, False]).reset_index()\n",
    "r = resorted['r'].round(2)\n",
    "r2 = resorted['r2'].round(2)\n",
    "resorted\n",
    "\n",
    "def custom_regplot(x, y, **kwargs):\n",
    "    label = kwargs.get('label')\n",
    "    sns.regplot(x=x, y=y ,\n",
    "                scatter_kws={'color':scatter_color[label], \"alpha\":0.5},\n",
    "                order=1,\n",
    "                line_kws={'color':line_color[label]}, **kwargs)\n",
    "\n",
    "g = sns.FacetGrid(mediandatatset.reset_index(), row='Platform',\n",
    "                  row_order=['Twitter','Reddit'],\n",
    "                    col='Theme', hue=\"party\", sharex=True, sharey=False, margin_titles=True,\n",
    "                    aspect=1.9)\n",
    "g.set_titles(col_template=\"{col_name}\\n\\n\", row_template=\"{row_name}\", size=20, fontweight='bold')\n",
    "\n",
    "g.map_dataframe(custom_regplot, y=\"entropyRound\", x=\"toxicity\", ci=95)\n",
    "\n",
    "g.figure.subplots_adjust(wspace=0.25, hspace=0.27, top=1.0)\n",
    "\n",
    "\n",
    "for text_obj in g._margin_titles_texts:\n",
    "    original_title = text_obj.get_text()  # 获取原始标题\n",
    "    text_obj.set_text(original_title.split()[0])  \n",
    "\n",
    "for index, ax in enumerate(g.axes.flat):\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", bottom=True, labelbottom=True)\n",
    "    # ax.tick_params(axis=\"y\", which=\"both\", left=True, labelleft=True)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.set_ylabel('')\n",
    "    if index in [0, 3]:\n",
    "        ax.set_ylabel('Entropy', fontweight='bold')\n",
    "    # if index in [6, 9]:\n",
    "    #     ax.set_ylabel('Median Pessimism')\n",
    "    ax.text(0, 1.15, letters[index], transform=ax.transAxes, \n",
    "        fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "    \n",
    "    ax.text(0.8, 1.03, fr\"$\\mathbf{{R^2}}$={r2[index*2]}\",transform=ax.transAxes, fontsize=14, fontweight='bold', va='top', ha='left', color='red')\n",
    "    ax.text(0.8, 1.13, fr\"$\\mathbf{{R^2}}$={r2[index*2+1]}\",transform=ax.transAxes, fontsize=14, fontweight='bold', va='top', ha='left', color='blue')\n",
    "      \n",
    "g.set_xlabels('Median language toxicity', fontweight='bold')\n",
    "\n",
    "g.add_legend(loc='upper center', label_order=['Republicans','Democrats'], title=\"\", bbox_to_anchor=(0.35, 1.32), ncol=2, fontsize=20, markerscale=2.5)\n",
    "g.savefig('Fig8.pdf', bbox_inches='tight')\n",
    "g.savefig('Fig8.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohensD(x1, x2):\n",
    "    mean1, mean2 = np.mean(x1), np.mean(x2)\n",
    "    std1 ,std2 = np.std(x1,ddof=1), np.std(x2, ddof=1)\n",
    "    n1 , n2 = len(x1), len(x2)\n",
    "    pooled_std = np.sqrt(((n1-1) * std1 ** 2 + (n2-1) * std2 ** 2) / (n1 + n2 -2))\n",
    "    d = (mean1 - mean2) / pooled_std\n",
    "    return d\n",
    "\n",
    "def dipt(x):\n",
    "    d, p = diptest.diptest(x)\n",
    "    return d, p\n",
    "\n",
    "def test(dataset, groupby, x1, y, values, alternative=\"greater\"):\n",
    "    return pd.concat(\n",
    "        [\n",
    "            dataset.groupby(groupby).apply(\n",
    "                lambda x: pd.Series(\n",
    "                    {\n",
    "                        \"Statistics\": \n",
    "                        stats.mannwhitneyu(x[x[x1] == y[0]][value], x[x[x1] == y[1]][value], alternative=alternative)[0],\n",
    "                        \"P-value\": \n",
    "                        stats.mannwhitneyu(x[x[x1] == y[0]][value], x[x[x1] == y[1]][value], alternative=alternative)[1],\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "        for value in values],\n",
    "        keys=values,\n",
    "    )\n",
    "\n",
    "lmDataset = dataset[['toxicity','pessimism','entropy','party','core_status','topic','Theme',\"Platform\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table S7 S9\n",
    "ret = pd.concat(\n",
    "    [\n",
    "        lmDataset.groupby(['Theme','Platform',\"core_status\"]).apply(\n",
    "        lambda x: pd.Series(\n",
    "            {\n",
    "            \"MW-u test(stat)\": \n",
    "            stats.mannwhitneyu(x[x['party'] == 'Democrats']['toxicity'].dropna(), x[x['party'] == \"Republicans\"]['toxicity'], alternative=\"less\")[0],\n",
    "            \"MW-u test(p)\": \n",
    "            stats.mannwhitneyu(x[x['party'] == 'Democrats']['toxicity'].dropna(), x[x['party'] == \"Republicans\"]['toxicity'], alternative=\"less\")[1],\n",
    "            \"cliff's Delta(stat)\":\n",
    "            cliffs_delta(x[x['party'] == 'Democrats']['toxicity'], x[x['party'] == \"Republicans\"]['toxicity'])[0],\n",
    "            \"cliff's Delta\":\n",
    "            cliffs_delta(x[x['party'] == 'Democrats']['toxicity'], x[x['party'] == \"Republicans\"]['toxicity'])[1],\n",
    "            \"Fligner-Killeen(stat)\":\n",
    "            stats.fligner(x[x['party'] == 'Democrats']['toxicity'], x[x['party'] == \"Republicans\"]['toxicity'])[0],\n",
    "            \"Fligner-Killeen(p)\":\n",
    "            stats.fligner(x[x['party'] == 'Democrats']['toxicity'], x[x['party'] == \"Republicans\"]['toxicity'])[1],     \n",
    "            })\n",
    "            ),\n",
    "        lmDataset.groupby(['Theme','Platform','party']).apply(\n",
    "        lambda x: pd.Series(\n",
    "            {\n",
    "            \"MW-u test(stat)\": \n",
    "            stats.mannwhitneyu(x[x['core_status'] == '1 degree']['toxicity'], x[x['core_status'] == '2 core']['toxicity'], alternative=\"less\")[0],\n",
    "            \"MW-u test(p)\": \n",
    "            stats.mannwhitneyu(x[x['core_status'] == '1 degree']['toxicity'], x[x['core_status'] == '2 core']['toxicity'], alternative=\"less\")[1],\n",
    "            \"cliff's Delta(stat)\":\n",
    "            cliffs_delta(x[x['core_status'] == '1 degree']['toxicity'], x[x['core_status'] == '2 core']['toxicity'])[0],\n",
    "            \"cliff's Delta\":\n",
    "            cliffs_delta(x[x['core_status'] == '1 degree']['toxicity'], x[x['core_status'] == '2 core']['toxicity'])[1],\n",
    "            \"Fligner-Killeen(stat)\":\n",
    "            stats.fligner(x[x['core_status'] == '1 degree']['toxicity'], x[x['core_status'] == '2 core']['toxicity'])[0],\n",
    "            \"Fligner-Killeen(p)\":\n",
    "            stats.fligner(x[x['core_status'] == '1 degree']['toxicity'], x[x['core_status'] == '2 core']['toxicity'])[1],     \n",
    "            })\n",
    "            ),\n",
    "    ],\n",
    "    keys=['Party','Core Stauts']\n",
    ")\n",
    "ret.to_csv(\"S7.csv\")\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table S8 S10\n",
    "ret = pd.concat(\n",
    "    [\n",
    "        lmDataset.groupby(['Theme','Platform',\"core_status\"]).apply(\n",
    "        lambda x: pd.Series(\n",
    "            {\n",
    "            \"MW-u test(stat)\": \n",
    "            stats.mannwhitneyu(x[x['party'] == 'Democrats']['pessimism'].dropna(), x[x['party'] == \"Republicans\"]['pessimism'], alternative=\"greater\")[0],\n",
    "            \"MW-u test(p)\": \n",
    "            stats.mannwhitneyu(x[x['party'] == 'Democrats']['pessimism'].dropna(), x[x['party'] == \"Republicans\"]['pessimism'], alternative=\"greater\")[1],\n",
    "            \"cliff's Delta(stat)\":\n",
    "            cliffs_delta(x[x['party'] == 'Democrats']['pessimism'], x[x['party'] == \"Republicans\"]['pessimism'])[0],\n",
    "            \"cliff's Delta\":\n",
    "            cliffs_delta(x[x['party'] == 'Democrats']['pessimism'], x[x['party'] == \"Republicans\"]['pessimism'])[1],\n",
    "            \"Fligner-Killeen(stat)\":\n",
    "            stats.fligner(x[x['party'] == 'Democrats']['pessimism'], x[x['party'] == \"Republicans\"]['pessimism'])[0],\n",
    "            \"Fligner-Killeen(p)\":\n",
    "            stats.fligner(x[x['party'] == 'Democrats']['pessimism'], x[x['party'] == \"Republicans\"]['pessimism'])[1],     \n",
    "            })\n",
    "            ),\n",
    "        lmDataset.groupby(['Theme','Platform','party']).apply(\n",
    "        lambda x: pd.Series(\n",
    "            {\n",
    "            \"MW-u test(stat)\": \n",
    "            stats.mannwhitneyu(x[x['core_status'] == '1 degree']['pessimism'], x[x['core_status'] == '2 core']['pessimism'], alternative=\"greater\")[0],\n",
    "            \"MW-u test(p)\": \n",
    "            stats.mannwhitneyu(x[x['core_status'] == '1 degree']['pessimism'], x[x['core_status'] == '2 core']['pessimism'], alternative=\"greater\")[1],\n",
    "            \"cliff's Delta(stat)\":\n",
    "            cliffs_delta(x[x['core_status'] == '1 degree']['pessimism'], x[x['core_status'] == '2 core']['pessimism'])[0],\n",
    "            \"cliff's Delta\":\n",
    "            cliffs_delta(x[x['core_status'] == '1 degree']['pessimism'], x[x['core_status'] == '2 core']['pessimism'])[1],\n",
    "            \"Fligner-Killeen(stat)\":\n",
    "            stats.fligner(x[x['core_status'] == '1 degree']['pessimism'], x[x['core_status'] == '2 core']['pessimism'])[0],\n",
    "            \"Fligner-Killeen(p)\":\n",
    "            stats.fligner(x[x['core_status'] == '1 degree']['pessimism'], x[x['core_status'] == '2 core']['pessimism'])[1],     \n",
    "            })\n",
    "            ),\n",
    "    ],\n",
    "    keys=['Party','Core Stauts']\n",
    ")\n",
    "ret.to_csv(\"S8.csv\")\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table S12\n",
    "ret = test(lmDataset, [\"Platform\", \"Theme\"], 'party', [\"Republicans\", \"Democrats\"], [\"toxicity\"])\n",
    "ret.to_csv(\"S12.csv\")\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table S13\n",
    "ret = pd.concat(\n",
    "    [\n",
    "        inDataset.groupby(['Theme',\"party\"]).apply(\n",
    "        lambda x: pd.Series(\n",
    "            {\n",
    "            \"MW-u test(stat)\": \n",
    "            stats.mannwhitneyu(x[x['Platform'] == 'Reddit']['toxicity'], x[x['Platform'] == \"Twitter\"]['toxicity'], alternative=\"greater\")[0],\n",
    "            \"MW-u test(p)\": \n",
    "            stats.mannwhitneyu(x[x['Platform'] == 'Reddit']['toxicity'], x[x['Platform'] == \"Twitter\"]['toxicity'], alternative=\"greater\")[1],\n",
    "            \"cliff's Delta(stat)\":\n",
    "            cliffs_delta(x[x['Platform'] == 'Reddit']['toxicity'], x[x['Platform'] == \"Twitter\"]['toxicity'])[0],\n",
    "            \"cliff's Delta\":\n",
    "            cliffs_delta(x[x['Platform'] == 'Reddit']['toxicity'], x[x['Platform'] == \"Twitter\"]['toxicity'])[1],\n",
    "            \"kstest\":\n",
    "            stats.kstest(x[x['Platform'] == 'Reddit']['toxicity'], x[x['Platform'] == \"Twitter\"]['toxicity'])[0],\n",
    "            \"ks(p)\":\n",
    "            stats.kstest(x[x['Platform'] == 'Reddit']['toxicity'], x[x['Platform'] == \"Twitter\"]['toxicity'])[1],   \n",
    "            })\n",
    "            ),\n",
    "        lmDataset.groupby(['Theme','party']).apply(\n",
    "        lambda x: pd.Series(\n",
    "            {\n",
    "            \"MW-u test(stat)\": \n",
    "            stats.mannwhitneyu(x[x['Platform'] == 'Reddit']['pessimism'], x[x['Platform'] == 'Twitter']['pessimism'], alternative=\"greater\")[0],\n",
    "            \"MW-u test(p)\": \n",
    "            stats.mannwhitneyu(x[x['Platform'] == 'Reddit']['pessimism'], x[x['Platform'] == 'Twitter']['pessimism'], alternative=\"greater\")[1],\n",
    "            \"cliff's Delta(stat)\":\n",
    "            cliffs_delta(x[x['Platform'] == 'Reddit']['pessimism'], x[x['Platform'] == 'Twitter']['pessimism'])[0],\n",
    "            \"cliff's Delta\":\n",
    "            cliffs_delta(x[x['Platform'] == 'Reddit']['pessimism'], x[x['Platform'] == 'Twitter']['pessimism'])[1],\n",
    "            \"kstest\":\n",
    "            stats.kstest(x[x['Platform'] == 'Reddit']['pessimism'], x[x['Platform'] == \"Twitter\"]['pessimism'])[0],\n",
    "            \"ks(p)\":\n",
    "            stats.kstest(x[x['Platform'] == 'Reddit']['pessimism'], x[x['Platform'] == \"Twitter\"]['pessimism'])[1],  \n",
    "            })\n",
    "            ),\n",
    "        lmDataset.groupby([\"Theme\",\"party\"]).apply(\n",
    "        lambda x: pd.Series(\n",
    "            {\n",
    "                \"MW-u test(stat)\": \n",
    "                stats.mannwhitneyu(x[x['Platform'] == 'Reddit']['entropy'].dropna(), x[x['Platform'] == \"Twitter\"]['entropy'], alternative=\"greater\")[0],\n",
    "                \"MW-u test(p)\": \n",
    "                stats.mannwhitneyu(x[x['Platform'] == 'Reddit']['entropy'].dropna(), x[x['Platform'] == \"Twitter\"]['entropy'], alternative=\"greater\")[1],\n",
    "                \"cliff's Delta(stat)\":\n",
    "                cliffs_delta(x[x['Platform'] == 'Reddit']['entropy'].dropna(), x[x['Platform'] == \"Twitter\"]['entropy'].dropna())[0],\n",
    "                \"cliff's Delta\":\n",
    "                cliffs_delta(x[x['Platform'] == 'Reddit']['entropy'].dropna(), x[x['Platform'] == \"Twitter\"]['entropy'].dropna())[1],  \n",
    "                \"kstest\":\n",
    "                stats.kstest(x[x['Platform'] == 'Reddit']['entropy'].dropna(), x[x['Platform'] == \"Twitter\"]['entropy'])[0],\n",
    "                \"ks(p)\":\n",
    "                stats.kstest(x[x['Platform'] == 'Reddit']['entropy'].dropna(), x[x['Platform'] == \"Twitter\"]['entropy'])[1],\n",
    "            })\n",
    "        )\n",
    "    ],\n",
    "    keys=['Toxicity','Pessimism', \"Entropy\"]\n",
    ")\n",
    "ret.to_csv(\"S13.csv\")\n",
    "ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
