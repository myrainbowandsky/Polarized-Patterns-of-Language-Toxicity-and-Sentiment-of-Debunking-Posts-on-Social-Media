{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time slice analysis\n",
    "\n",
    "python=3.7\n",
    "|topic      |platform   |language   |\n",
    "|-----------|-----------|-----------|\n",
    "|QAnon      |Reddit     |en         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load debunking community (debunking dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14637, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_debunk = pd.read_csv(\"data/debunking_comments[keyword=QAnon][lang=en].csv\")\n",
    "df_debunk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\\\n', '', text) \n",
    "    text = re.sub('\\[.*?\\]', '', text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) \n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('<.*?>+', '', text) \n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) \n",
    "    text = re.sub('\\n', '', text) \n",
    "    text = re.sub('\\w*\\d\\w*', '', text) \n",
    "    return text\n",
    "\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "eng_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "def remove_eng_stopwords(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    remove_stop = [word for word in token_text if word not in eng_stopwords]\n",
    "    join_text = ' '.join(remove_stop)\n",
    "    return join_text\n",
    "\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "def word_lemmatizer(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    remove_stop = [lemm.lemmatize(w) for w in token_text]\n",
    "    join_text = ' '.join(remove_stop)\n",
    "    return join_text\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "Word_STOPWORDS = [\"e\", \"te\", \"i\", \"me\", \"qe\", \"ne\", \"nje\", \"a\", \"per\", \"sh\", \"nga\", \"ka\", \"u\", \"eshte\", \"dhe\", \"shih\", \"nuk\",\n",
    "             \"m\", \"dicka\", \"ose\", \"si\", \"shume\", \"etj\", \"se\", \"pa\", \"sipas\", \"s\", \"t\", \"dikujt\", \"dike\", \"mire\", \"vet\",\n",
    "             \"bej\", \"ai\", \"vend\", \"prej\", \"ja\", \"duke\", \"tjeter\", \"kur\", \"ia\", \"ku\", \"ta\", \"keq\", \"dy\", \"ben\", \"bere\",\n",
    "             \"behet\", \"dickaje\", \"edhe\", \"madhe\", \"la\", \"sa\", \"gjate\", \"zakonisht\", \"pas\", \"veta\", \"mbi\", \"disa\", \"iu\",\n",
    "             \"mos\", \"c\", \"para\", \"dikush\", \"gje\", \"be\", \"pak\", \"tek\", \"fare\", \"beri\", \"po\", \"bie\", \"k\", \"do\", \"gjithe\",\n",
    "             \"vete\", \"mund\", \"kam\", \"le\", \"jo\", \"beje\", \"tij\", \"kane\", \"ishte\", \"jane\", \"vjen\", \"ate\", \"kete\", \"neper\",\n",
    "             \"cdo\", \"na\", \"marre\", \"merr\", \"mori\", \"rri\", \"deri\", \"b\", \"kishte\", \"mban\", \"perpara\", \"tyre\", \"marr\",\n",
    "             \"gjitha\", \"as\", \"vetem\", \"nen\", \"here\", \"tjera\", \"tjeret\", \"drejt\", \"qenet\", \"ndonje\", \"nese\", \"jap\",\n",
    "             \"merret\", \"rreth\", \"lloj\", \"dot\", \"saj\", \"nder\", \"ndersa\", \"cila\", \"veten\", \"ma\", \"ndaj\", \"mes\", \"ajo\",\n",
    "             \"cilen\", \"por\", \"ndermjet\", \"prapa\", \"mi\", \"tere\", \"jam\", \"ashtu\", \"kesaj\", \"tille\", \"behem\", \"cilat\",\n",
    "             \"kjo\", \"menjehere\", \"ca\", \"je\", \"aq\", \"aty\", \"prane\", \"ato\", \"pasur\", \"qene\", \"cilin\", \"teper\", \"njera\",\n",
    "             \"tej\", \"krejt\", \"kush\", \"bejne\", \"ti\", \"bene\", \"midis\", \"cili\", \"ende\", \"keto\", \"kemi\", \"sic\", \"kryer\",\n",
    "             \"cilit\", \"atij\", \"gjithnje\", \"andej\", \"siper\", \"sikur\", \"ketej\", \"ciles\", \"ky\", \"papritur\", \"ua\",\n",
    "             \"kryesisht\", \"gjithcka\", \"pasi\", \"kryhet\", \"mjaft\", \"ketij\", \"perbashket\", \"ata\", \"atje\", \"vazhdimisht\",\n",
    "             \"kurre\", \"tone\", \"keshtu\", \"une\", \"sapo\", \"rralle\", \"vetes\", \"ishin\", \"afert\", \"tjetren\", \"ketu\", \"cfare\",\n",
    "             \"to\", \"anes\", \"jemi\", \"asaj\", \"secila\", \"kundrejt\", \"ketyre\", \"pse\", \"tilla\", \"mua\", \"nepermjet\", \"cilet\",\n",
    "             \"ndryshe\", \"kishin\", \"ju\", \"tani\", \"atyre\", \"dic\", \"yne\", \"kudo\", \"sone\", \"sepse\", \"cilave\", \"kem\", \"ty\",\n",
    "             \"t'i\", \"nbsp\", \"tha\", \"re\", \"the\", \"jr\", \"t\", \"n\"]\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "text_unknows= Word_STOPWORDS\n",
    "stop.update(text_unknows)\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    '''Removing the square brackets'''\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    '''Removing URL's'''\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''Removing the stopwords from text'''\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    '''Removing the noisy text'''\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = wordopt(text)\n",
    "    text = remove_eng_stopwords(text)\n",
    "    text = word_lemmatizer(text)\n",
    "    text = denoise_text(text)\n",
    "    text = punctuation_removal(text)\n",
    "    return text\n",
    "\n",
    "df_txt = df_debunk\n",
    "df_txt['body'] = df_txt['body'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'created_at' column to datetime\n",
    "df_txt['created_utc'] = pd.to_datetime(df_txt['created_utc'])\n",
    "# Extract date from 'created_at' column\n",
    "df_txt['date'] = df_txt['created_utc'].dt.date\n",
    "# Group by date\n",
    "grouped_df = df_txt.groupby('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each time slice, aggregate texts for each user\n",
    "time_slices = dict()\n",
    "for name, group in grouped_df:\n",
    "    time_slices[name] = group.groupby(by='author').agg(text=(\"body\", lambda x: ' '.join(set(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time_slices from dict into DataFrame, then save it as csv\n",
    "for k, v in time_slices.items():\n",
    "    v['date'] = k\n",
    "\n",
    "df_merge_slices = pd.concat(time_slices.values())\n",
    "df_merge_slices.sort_values(by='date', inplace=True)\n",
    "df_merge_slices.to_csv(\"data/time_slices[topic=QAnon][platform=Reddit][lang=en][debunking=keywords].csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13514 entries, 0 to 13513\n",
      "Data columns (total 4 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   author                   13514 non-null  object\n",
      " 1   text                     13482 non-null  object\n",
      " 2   date                     13514 non-null  object\n",
      " 3   perspective_api_results  13455 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 422.4+ KB\n"
     ]
    }
   ],
   "source": [
    "perspective_path = \"data/toxicity_of_time_slices[topic=QAnon][platform=Reddit][lang=en][debunking=keywords].csv\"\n",
    "perspective_res = pd.read_csv(perspective_path)\n",
    "perspective_res.info()\n",
    "\n",
    "def get_score_from_json(x):\n",
    "   \n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = re.search(\"'score': {'value': (.+?),\", x)\n",
    "    return float(s.group(1))\n",
    "\n",
    "perspective_res['toxicity'] = perspective_res['perspective_api_results'].apply(get_score_from_json)\n",
    "perspective_res.to_csv(perspective_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiments detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import liwc\n",
    "liwcPath = r'data/LIWC2015_English.dic'\n",
    "parse, category_names = liwc.load_token_parser(liwcPath)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def liwc_analyse_ver2(text, categories=['positive','negative','affect']):\n",
    "    corpus = []\n",
    "    words = []\n",
    "\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    review = review.split()\n",
    "    review = list(category for token in review for category in parse(token))\n",
    "    statements = ' '.join(review)\n",
    "    corpus.append(statements)\n",
    "    words.append(review)\n",
    "    \n",
    "    # TF-IDF\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        X_fit = vectorizer.fit(corpus)\n",
    "        X_transformed = X_fit.transform(corpus)\n",
    "\n",
    "        features = vectorizer.get_feature_names()\n",
    "        df = pd.DataFrame(X_transformed.toarray(),columns=features)\n",
    "        result = {col: df.get(col) for col in categories}\n",
    "        result_df = pd.DataFrame(result)\n",
    "    except:\n",
    "        result_df = pd.DataFrame({k:[None] for k in categories})\n",
    "\n",
    "    return result_df.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af407c7435844fcfa89ec8b4ce3663e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1352), Label(value='0 / 1352'))), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=10)\n",
    "selected_categories = ['positive','negative','affect']\n",
    "perspective_res.loc[:, selected_categories] = perspective_res['text'].astype(str).parallel_apply(liwc_analyse_ver2)\n",
    "perspective_res.to_csv(\"data/time_slices[topic=QAnon][platform=Reddit][lang=en][debunking=keywords].csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize daily datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(df:pd.DataFrame):\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    scores = ['positive', 'negative', 'affect', 'toxicity']\n",
    "\n",
    "    df_groupby_date = df.groupby('date')\n",
    "\n",
    "\n",
    "    daily_user_count = df_groupby_date['author'].nunique()\n",
    "\n",
    "    score_none_count = df_groupby_date[scores].apply(lambda x: x.isnull().sum())\n",
    "\n",
    "    daily_mean = df_groupby_date[scores].mean()\n",
    "    daily_median = df_groupby_date[scores].median()\n",
    "\n",
    " \n",
    "    def mean_no_extreme(df:pd.DataFrame):\n",
    "        q1 = df[scores].quantile(0.25)\n",
    "        q3 = df[scores].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        df_no_extreme = df[~((df[scores] < (q1 - 1.5 * iqr)) | (df[scores] > (q3 + 1.5 * iqr)))]\n",
    "        return df_no_extreme[scores].mean()\n",
    "    \n",
    "    daily_mean_no_extreme = df_groupby_date.apply(mean_no_extreme)\n",
    "\n",
    " \n",
    "    daily_data = pd.DataFrame({\n",
    "        'date': daily_user_count.index,\n",
    "        'user_count': daily_user_count.values,\n",
    "        'positive_none_count': score_none_count['positive'],\n",
    "        'negative_none_count': score_none_count['negative'],\n",
    "        'affect_none_count': score_none_count['affect'],\n",
    "        'toxicity_none_count': score_none_count['toxicity'],\n",
    "        'positive_mean': daily_mean['positive'].values,\n",
    "        'negative_mean': daily_mean['negative'].values,\n",
    "        'affect_mean': daily_mean['affect'].values,\n",
    "        'toxicity_mean': daily_mean['toxicity'].values,\n",
    "        'positive_median': daily_median['positive'].values,\n",
    "        'negative_median': daily_median['negative'].values,\n",
    "        'affect_median': daily_median['affect'].values,\n",
    "        'toxicity_median': daily_median['toxicity'].values,\n",
    "        'positive_mean_no_extreme': daily_mean_no_extreme['positive'].values,\n",
    "        'negative_mean_no_extreme': daily_mean_no_extreme['negative'].values,\n",
    "        'affect_mean_no_extreme': daily_mean_no_extreme['affect'].values,\n",
    "        'toxicity_mean_no_extreme': daily_mean_no_extreme['toxicity'].values,\n",
    "    })\n",
    "\n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_count</th>\n",
       "      <th>positive_none_count</th>\n",
       "      <th>negative_none_count</th>\n",
       "      <th>affect_none_count</th>\n",
       "      <th>toxicity_none_count</th>\n",
       "      <th>positive_mean</th>\n",
       "      <th>negative_mean</th>\n",
       "      <th>affect_mean</th>\n",
       "      <th>toxicity_mean</th>\n",
       "      <th>positive_median</th>\n",
       "      <th>negative_median</th>\n",
       "      <th>affect_median</th>\n",
       "      <th>toxicity_median</th>\n",
       "      <th>positive_mean_no_extreme</th>\n",
       "      <th>negative_mean_no_extreme</th>\n",
       "      <th>affect_mean_no_extreme</th>\n",
       "      <th>toxicity_mean_no_extreme</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.099086</td>\n",
       "      <td>0.265188</td>\n",
       "      <td>0.568845</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.041772</td>\n",
       "      <td>0.226762</td>\n",
       "      <td>0.698991</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.099086</td>\n",
       "      <td>0.265188</td>\n",
       "      <td>0.568845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.122699</td>\n",
       "      <td>0.085547</td>\n",
       "      <td>0.332551</td>\n",
       "      <td>0.304634</td>\n",
       "      <td>0.090224</td>\n",
       "      <td>0.100261</td>\n",
       "      <td>0.280731</td>\n",
       "      <td>0.284514</td>\n",
       "      <td>0.122699</td>\n",
       "      <td>0.085547</td>\n",
       "      <td>0.332551</td>\n",
       "      <td>0.226146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-03</th>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077810</td>\n",
       "      <td>0.076313</td>\n",
       "      <td>0.243922</td>\n",
       "      <td>0.269872</td>\n",
       "      <td>0.074261</td>\n",
       "      <td>0.080224</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>0.282002</td>\n",
       "      <td>0.077810</td>\n",
       "      <td>0.076313</td>\n",
       "      <td>0.243922</td>\n",
       "      <td>0.169905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-04</th>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071862</td>\n",
       "      <td>0.081377</td>\n",
       "      <td>0.261261</td>\n",
       "      <td>0.351190</td>\n",
       "      <td>0.064778</td>\n",
       "      <td>0.053082</td>\n",
       "      <td>0.265408</td>\n",
       "      <td>0.375766</td>\n",
       "      <td>0.071862</td>\n",
       "      <td>0.064317</td>\n",
       "      <td>0.241037</td>\n",
       "      <td>0.351190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-05</th>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.053813</td>\n",
       "      <td>0.077854</td>\n",
       "      <td>0.248516</td>\n",
       "      <td>0.378525</td>\n",
       "      <td>0.046790</td>\n",
       "      <td>0.074310</td>\n",
       "      <td>0.217437</td>\n",
       "      <td>0.412433</td>\n",
       "      <td>0.053813</td>\n",
       "      <td>0.077854</td>\n",
       "      <td>0.248516</td>\n",
       "      <td>0.378525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  user_count  positive_none_count  negative_none_count  \\\n",
       "date                                                                          \n",
       "2020-04-01 2020-04-01           3                    1                    0   \n",
       "2020-04-02 2020-04-02           8                    2                    3   \n",
       "2020-04-03 2020-04-03           7                    0                    3   \n",
       "2020-04-04 2020-04-04          13                    2                    2   \n",
       "2020-04-05 2020-04-05           6                    1                    0   \n",
       "\n",
       "            affect_none_count  toxicity_none_count  positive_mean  \\\n",
       "date                                                                \n",
       "2020-04-01                  0                    0       0.042459   \n",
       "2020-04-02                  1                    0       0.122699   \n",
       "2020-04-03                  0                    0       0.077810   \n",
       "2020-04-04                  0                    0       0.071862   \n",
       "2020-04-05                  0                    0       0.053813   \n",
       "\n",
       "            negative_mean  affect_mean  toxicity_mean  positive_median  \\\n",
       "date                                                                     \n",
       "2020-04-01       0.099086     0.265188       0.568845         0.042459   \n",
       "2020-04-02       0.085547     0.332551       0.304634         0.090224   \n",
       "2020-04-03       0.076313     0.243922       0.269872         0.074261   \n",
       "2020-04-04       0.081377     0.261261       0.351190         0.064778   \n",
       "2020-04-05       0.077854     0.248516       0.378525         0.046790   \n",
       "\n",
       "            negative_median  affect_median  toxicity_median  \\\n",
       "date                                                          \n",
       "2020-04-01         0.041772       0.226762         0.698991   \n",
       "2020-04-02         0.100261       0.280731         0.284514   \n",
       "2020-04-03         0.080224       0.235702         0.282002   \n",
       "2020-04-04         0.053082       0.265408         0.375766   \n",
       "2020-04-05         0.074310       0.217437         0.412433   \n",
       "\n",
       "            positive_mean_no_extreme  negative_mean_no_extreme  \\\n",
       "date                                                             \n",
       "2020-04-01                  0.042459                  0.099086   \n",
       "2020-04-02                  0.122699                  0.085547   \n",
       "2020-04-03                  0.077810                  0.076313   \n",
       "2020-04-04                  0.071862                  0.064317   \n",
       "2020-04-05                  0.053813                  0.077854   \n",
       "\n",
       "            affect_mean_no_extreme  toxicity_mean_no_extreme  \n",
       "date                                                          \n",
       "2020-04-01                0.265188                  0.568845  \n",
       "2020-04-02                0.332551                  0.226146  \n",
       "2020-04-03                0.243922                  0.169905  \n",
       "2020-04-04                0.241037                  0.351190  \n",
       "2020-04-05                0.248516                  0.378525  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/time_slices[topic=QAnon][platform=Reddit][lang=en][debunking=keywords].csv\")\n",
    "daily_statistics = calculate_statistics(data)\n",
    "daily_statistics.to_csv(\"data/daily_statistics[topic=QAnon][platform=Reddit][lang=en][debunking=keywords].csv\", index=False)\n",
    "daily_statistics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 396 entries, 2020-04-01 to 2021-05-01\n",
      "Data columns (total 18 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   date                      396 non-null    datetime64[ns]\n",
      " 1   user_count                396 non-null    int64         \n",
      " 2   positive_none_count       396 non-null    int64         \n",
      " 3   negative_none_count       396 non-null    int64         \n",
      " 4   affect_none_count         396 non-null    int64         \n",
      " 5   toxicity_none_count       396 non-null    int64         \n",
      " 6   positive_mean             396 non-null    float64       \n",
      " 7   negative_mean             396 non-null    float64       \n",
      " 8   affect_mean               396 non-null    float64       \n",
      " 9   toxicity_mean             396 non-null    float64       \n",
      " 10  positive_median           396 non-null    float64       \n",
      " 11  negative_median           396 non-null    float64       \n",
      " 12  affect_median             396 non-null    float64       \n",
      " 13  toxicity_median           396 non-null    float64       \n",
      " 14  positive_mean_no_extreme  396 non-null    float64       \n",
      " 15  negative_mean_no_extreme  396 non-null    float64       \n",
      " 16  affect_mean_no_extreme    396 non-null    float64       \n",
      " 17  toxicity_mean_no_extreme  396 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(12), int64(5)\n",
      "memory usage: 58.8 KB\n"
     ]
    }
   ],
   "source": [
    "daily_statistics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n"
     ]
    }
   ],
   "source": [
    "# Calculate days from the earliest date to the latest\n",
    "delta = daily_statistics['date'][-1] - daily_statistics['date'][0]\n",
    "print(delta.days + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
