{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time slice analysis\n",
    "\n",
    "python=3.7\n",
    "|topic      |platform   |language   |\n",
    "|-----------|-----------|-----------|\n",
    "|COVID-19   |Reddit     |en         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 输出DataFrame时显示所有的列\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 输出DataFrame时每行显示完整的内容\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "name_suffix = \"[debunking=keywords][lang=en][topic=POTUS2016][platform=Reddit]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load debunking community (debunking dataset)\n",
    "\n",
    "This step is solely for extracting the debunking dataset; if you already have a saved debunking dataset file, there's no need to go through the cumbersome process as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 157917 entries, 0 to 157916\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   created_utc   157917 non-null  object\n",
      " 1   author        157917 non-null  object\n",
      " 2   subreddit     157917 non-null  object\n",
      " 3   body          157917 non-null  object\n",
      " 4   parent_id     157915 non-null  object\n",
      " 5   subreddit_id  157915 non-null  object\n",
      " 6   id            157915 non-null  object\n",
      " 7   lang          157917 non-null  object\n",
      " 8   body_cleaned  156896 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 10.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_debunk = pd.read_csv(f\"data/debunking_comments{name_suffix}.csv\")  \n",
    "df_debunk.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'created_at' column to datetime\n",
    "df_debunk['created_utc'] = pd.to_datetime(df_debunk['created_utc'])\n",
    "# Extract date from 'created_at' column\n",
    "df_debunk['date'] = df_debunk['created_utc'].dt.date\n",
    "# Group by date\n",
    "df_debunk['body_cleaned'] = df_debunk['body_cleaned'].astype(str) \n",
    "grouped_df = df_debunk.groupby('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each time slice, aggregate texts for each user\n",
    "time_slices = dict()\n",
    "for name, group in grouped_df:\n",
    "    time_slices[name] = group.groupby(by='author').agg(text=(\"body_cleaned\", lambda x: ' '.join(set(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134326"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert time_slices from dict into DataFrame, then save it as csv\n",
    "for k, v in time_slices.items():\n",
    "    v['date'] = k\n",
    "\n",
    "df_merge_slices = pd.concat(time_slices.values())\n",
    "df_merge_slices.sort_values(by='date', inplace=True)\n",
    "df_merge_slices.to_csv(f\"data/time_slices{name_suffix}.csv\")\n",
    "len(df_merge_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just an example.\n",
    "data_path = f\"data/time_slices{name_suffix}.csv\"\n",
    "result_path = f\"data/toxicity_of_time_slices{name_suffix}.csv\"\n",
    "cmd = f\"python Perspective.py --data={data_path} --result={result_path} --max_workers=\" \n",
    "print(cmd)\n",
    "! {cmd} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perspective_path = f\"data/toxicity_of_time_slices{name_suffix}.csv\"\n",
    "perspective_res = pd.read_csv(perspective_path)\n",
    "perspective_res.info()\n",
    "\n",
    "def get_score_from_json(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = re.search(\"'score': {'value': (.+?),\", x)\n",
    "    return float(s.group(1))\n",
    "\n",
    "perspective_res['toxicity'] = perspective_res['perspective_api_results'].apply(get_score_from_json)\n",
    "perspective_res.to_csv(perspective_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiments detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the LIWC dictionary.\n",
    "import liwc\n",
    "liwcPath = r'data/LIWC2015_English.dic'\n",
    "parse, category_names = liwc.load_token_parser(liwcPath)\n",
    "\n",
    "# Analyze each user using LIWC.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def liwc_analyse_ver2(text, categories=['positive','negative','affect']):\n",
    "    corpus = []\n",
    "    words = []\n",
    "\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    review = review.split()\n",
    "    review = list(category for token in review for category in parse(token))\n",
    "    statements = ' '.join(review)\n",
    "    corpus.append(statements)\n",
    "    words.append(review)\n",
    "    \n",
    "    # TF-IDF\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        X_fit = vectorizer.fit(corpus)\n",
    "        X_transformed = X_fit.transform(corpus)\n",
    "\n",
    "        features = vectorizer.get_feature_names()\n",
    "        df = pd.DataFrame(X_transformed.toarray(),columns=features)\n",
    "        result = {col: df.get(col) for col in categories}\n",
    "        result_df = pd.DataFrame(result)\n",
    "    except:\n",
    "        result_df = pd.DataFrame({k:[None] for k in categories})\n",
    "\n",
    "    return result_df.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=10)\n",
    "selected_categories = ['positive','negative','affect']\n",
    "perspective_res.loc[:, selected_categories] = perspective_res['text'].astype(str).parallel_apply(liwc_analyse_ver2)\n",
    "perspective_res.to_csv(f\"data/time_slices{name_suffix}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize daily datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(df:pd.DataFrame):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    scores = ['positive', 'negative', 'affect', 'toxicity']\n",
    "    df_groupby_date = df.groupby('date')\n",
    "\n",
    "    daily_user_count = df_groupby_date['author'].nunique()\n",
    "    score_none_count = df_groupby_date[scores].apply(lambda x: x.isnull().sum())\n",
    "    daily_mean = df_groupby_date[scores].mean()\n",
    "    daily_median = df_groupby_date[scores].median()\n",
    "\n",
    "    def mean_no_extreme(df:pd.DataFrame):\n",
    "        q1 = df[scores].quantile(0.25)\n",
    "        q3 = df[scores].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        df_no_extreme = df[~((df[scores] < (q1 - 1.5 * iqr)) | (df[scores] > (q3 + 1.5 * iqr)))]\n",
    "        return df_no_extreme[scores].mean()\n",
    "    \n",
    "    daily_mean_no_extreme = df_groupby_date.apply(mean_no_extreme)\n",
    "\n",
    "    daily_data = pd.DataFrame({\n",
    "        'date': daily_user_count.index,\n",
    "        'user_count': daily_user_count.values,\n",
    "        'positive_none_count': score_none_count['positive'],\n",
    "        'negative_none_count': score_none_count['negative'],\n",
    "        'affect_none_count': score_none_count['affect'],\n",
    "        'toxicity_none_count': score_none_count['toxicity'],\n",
    "        'positive_mean': daily_mean['positive'].values,\n",
    "        'negative_mean': daily_mean['negative'].values,\n",
    "        'affect_mean': daily_mean['affect'].values,\n",
    "        'toxicity_mean': daily_mean['toxicity'].values,\n",
    "        'positive_median': daily_median['positive'].values,\n",
    "        'negative_median': daily_median['negative'].values,\n",
    "        'affect_median': daily_median['affect'].values,\n",
    "        'toxicity_median': daily_median['toxicity'].values,\n",
    "        'positive_mean_no_extreme': daily_mean_no_extreme['positive'].values,\n",
    "        'negative_mean_no_extreme': daily_mean_no_extreme['negative'].values,\n",
    "        'affect_mean_no_extreme': daily_mean_no_extreme['affect'].values,\n",
    "        'toxicity_mean_no_extreme': daily_mean_no_extreme['toxicity'].values,\n",
    "    })\n",
    "\n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_count</th>\n",
       "      <th>positive_none_count</th>\n",
       "      <th>negative_none_count</th>\n",
       "      <th>affect_none_count</th>\n",
       "      <th>toxicity_none_count</th>\n",
       "      <th>positive_mean</th>\n",
       "      <th>negative_mean</th>\n",
       "      <th>affect_mean</th>\n",
       "      <th>toxicity_mean</th>\n",
       "      <th>positive_median</th>\n",
       "      <th>negative_median</th>\n",
       "      <th>affect_median</th>\n",
       "      <th>toxicity_median</th>\n",
       "      <th>positive_mean_no_extreme</th>\n",
       "      <th>negative_mean_no_extreme</th>\n",
       "      <th>affect_mean_no_extreme</th>\n",
       "      <th>toxicity_mean_no_extreme</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-01</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>181</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063185</td>\n",
       "      <td>0.091573</td>\n",
       "      <td>0.268284</td>\n",
       "      <td>0.291490</td>\n",
       "      <td>0.053529</td>\n",
       "      <td>0.081781</td>\n",
       "      <td>0.247226</td>\n",
       "      <td>0.237760</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>0.084943</td>\n",
       "      <td>0.258914</td>\n",
       "      <td>0.244027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-02</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>259</td>\n",
       "      <td>80</td>\n",
       "      <td>46</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.065183</td>\n",
       "      <td>0.084390</td>\n",
       "      <td>0.256636</td>\n",
       "      <td>0.261620</td>\n",
       "      <td>0.055258</td>\n",
       "      <td>0.069285</td>\n",
       "      <td>0.239808</td>\n",
       "      <td>0.207059</td>\n",
       "      <td>0.061969</td>\n",
       "      <td>0.079073</td>\n",
       "      <td>0.246584</td>\n",
       "      <td>0.221080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-03</th>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>313</td>\n",
       "      <td>85</td>\n",
       "      <td>61</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.070109</td>\n",
       "      <td>0.085433</td>\n",
       "      <td>0.269941</td>\n",
       "      <td>0.273372</td>\n",
       "      <td>0.064541</td>\n",
       "      <td>0.074881</td>\n",
       "      <td>0.261262</td>\n",
       "      <td>0.205721</td>\n",
       "      <td>0.064323</td>\n",
       "      <td>0.080113</td>\n",
       "      <td>0.268339</td>\n",
       "      <td>0.240581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>294</td>\n",
       "      <td>69</td>\n",
       "      <td>62</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.070976</td>\n",
       "      <td>0.082456</td>\n",
       "      <td>0.263715</td>\n",
       "      <td>0.247207</td>\n",
       "      <td>0.064167</td>\n",
       "      <td>0.072071</td>\n",
       "      <td>0.246670</td>\n",
       "      <td>0.198934</td>\n",
       "      <td>0.066370</td>\n",
       "      <td>0.074573</td>\n",
       "      <td>0.257765</td>\n",
       "      <td>0.206991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>267</td>\n",
       "      <td>76</td>\n",
       "      <td>66</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>0.063178</td>\n",
       "      <td>0.083012</td>\n",
       "      <td>0.250254</td>\n",
       "      <td>0.244104</td>\n",
       "      <td>0.052007</td>\n",
       "      <td>0.076492</td>\n",
       "      <td>0.240719</td>\n",
       "      <td>0.188392</td>\n",
       "      <td>0.057321</td>\n",
       "      <td>0.078387</td>\n",
       "      <td>0.247099</td>\n",
       "      <td>0.215128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  user_count  positive_none_count  negative_none_count  \\\n",
       "date                                                                          \n",
       "2020-03-01 2020-03-01         181                   40                   33   \n",
       "2020-03-02 2020-03-02         259                   80                   46   \n",
       "2020-03-03 2020-03-03         313                   85                   61   \n",
       "2020-03-04 2020-03-04         294                   69                   62   \n",
       "2020-03-05 2020-03-05         267                   76                   66   \n",
       "\n",
       "            affect_none_count  toxicity_none_count  positive_mean  \\\n",
       "date                                                                \n",
       "2020-03-01                 12                    1       0.063185   \n",
       "2020-03-02                 26                    0       0.065183   \n",
       "2020-03-03                 32                    2       0.070109   \n",
       "2020-03-04                 26                    1       0.070976   \n",
       "2020-03-05                 36                    5       0.063178   \n",
       "\n",
       "            negative_mean  affect_mean  toxicity_mean  positive_median  \\\n",
       "date                                                                     \n",
       "2020-03-01       0.091573     0.268284       0.291490         0.053529   \n",
       "2020-03-02       0.084390     0.256636       0.261620         0.055258   \n",
       "2020-03-03       0.085433     0.269941       0.273372         0.064541   \n",
       "2020-03-04       0.082456     0.263715       0.247207         0.064167   \n",
       "2020-03-05       0.083012     0.250254       0.244104         0.052007   \n",
       "\n",
       "            negative_median  affect_median  toxicity_median  \\\n",
       "date                                                          \n",
       "2020-03-01         0.081781       0.247226         0.237760   \n",
       "2020-03-02         0.069285       0.239808         0.207059   \n",
       "2020-03-03         0.074881       0.261262         0.205721   \n",
       "2020-03-04         0.072071       0.246670         0.198934   \n",
       "2020-03-05         0.076492       0.240719         0.188392   \n",
       "\n",
       "            positive_mean_no_extreme  negative_mean_no_extreme  \\\n",
       "date                                                             \n",
       "2020-03-01                  0.056634                  0.084943   \n",
       "2020-03-02                  0.061969                  0.079073   \n",
       "2020-03-03                  0.064323                  0.080113   \n",
       "2020-03-04                  0.066370                  0.074573   \n",
       "2020-03-05                  0.057321                  0.078387   \n",
       "\n",
       "            affect_mean_no_extreme  toxicity_mean_no_extreme  \n",
       "date                                                          \n",
       "2020-03-01                0.258914                  0.244027  \n",
       "2020-03-02                0.246584                  0.221080  \n",
       "2020-03-03                0.268339                  0.240581  \n",
       "2020-03-04                0.257765                  0.206991  \n",
       "2020-03-05                0.247099                  0.215128  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/time_slices[topic=COVID19][platform=Reddit][lang=en][debunking=keywords].csv\")\n",
    "daily_statistics = calculate_statistics(data)\n",
    "daily_statistics.to_csv(\"data/daily_statistics[topic=COVID19][platform=Reddit][lang=en][debunking=keywords].csv\", index=False)\n",
    "daily_statistics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 731 entries, 2020-03-01 to 2022-03-01\n",
      "Data columns (total 18 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   date                      731 non-null    datetime64[ns]\n",
      " 1   user_count                731 non-null    int64         \n",
      " 2   positive_none_count       731 non-null    int64         \n",
      " 3   negative_none_count       731 non-null    int64         \n",
      " 4   affect_none_count         731 non-null    int64         \n",
      " 5   toxicity_none_count       731 non-null    int64         \n",
      " 6   positive_mean             731 non-null    float64       \n",
      " 7   negative_mean             731 non-null    float64       \n",
      " 8   affect_mean               731 non-null    float64       \n",
      " 9   toxicity_mean             731 non-null    float64       \n",
      " 10  positive_median           731 non-null    float64       \n",
      " 11  negative_median           731 non-null    float64       \n",
      " 12  affect_median             731 non-null    float64       \n",
      " 13  toxicity_median           731 non-null    float64       \n",
      " 14  positive_mean_no_extreme  731 non-null    float64       \n",
      " 15  negative_mean_no_extreme  731 non-null    float64       \n",
      " 16  affect_mean_no_extreme    731 non-null    float64       \n",
      " 17  toxicity_mean_no_extreme  731 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(12), int64(5)\n",
      "memory usage: 108.5 KB\n"
     ]
    }
   ],
   "source": [
    "daily_statistics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731\n"
     ]
    }
   ],
   "source": [
    "# Calculate days from the earliest date to the latest\n",
    "delta = daily_statistics['date'][-1] - daily_statistics['date'][0]\n",
    "print(delta.days + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
